{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Requirement already satisfied: plotly==4.6.0 in c:\\users\\martina\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from plotly==4.6.0) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\martina\\anaconda3\\lib\\site-packages (from plotly==4.6.0) (1.12.0)\n",
      "Collecting chart-studio==1.0.0\n",
      "  Downloading chart_studio-1.0.0-py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\martina\\anaconda3\\lib\\site-packages (from chart-studio==1.0.0) (2.22.0)\n",
      "Requirement already satisfied: plotly in c:\\users\\martina\\anaconda3\\lib\\site-packages (from chart-studio==1.0.0) (4.6.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from chart-studio==1.0.0) (1.3.3)\n",
      "Requirement already satisfied: six in c:\\users\\martina\\anaconda3\\lib\\site-packages (from chart-studio==1.0.0) (1.12.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from requests->chart-studio==1.0.0) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from requests->chart-studio==1.0.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from requests->chart-studio==1.0.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\martina\\anaconda3\\lib\\site-packages (from requests->chart-studio==1.0.0) (2019.9.11)\n",
      "Installing collected packages: chart-studio\n",
      "  Attempting uninstall: chart-studio\n",
      "    Found existing installation: chart-studio 1.1.0\n",
      "    Uninstalling chart-studio-1.1.0:\n",
      "      Successfully uninstalled chart-studio-1.1.0\n",
      "Successfully installed chart-studio-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!conda install \"notebook>=5.3\" \"ipywidgets>=7.2\"\n",
    "!conda install nbformat==4.4.0\n",
    "!pip install plotly==4.6.0\n",
    "!pip install chart-studio==1.0.0\n",
    "#!conda install chart_studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "O165VvV_L2s-"
   },
   "source": [
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgW92cheKw0i"
   },
   "source": [
    "# **Data Spaces - Analysis of the Online Shoppers Purchasing Intention Dataset**\n",
    "**Martina Alutto, s265027**\n",
    "\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "The analysis that will be presented has been carried out on the dataset available online at the following link: https://archive.ics.uci.edu/ml/datasets/Online+Shoppers+Purchasing+Intention+Dataset.\n",
    "\n",
    "This dataset consists of feature vectors belonging to 12,330 online sessions.\n",
    "The dataset was formed so that each session would belong to a different user in a 1-year period to avoid any tendency to a specific campaign, special day, user\n",
    "profile, or period. Some of these sessions on the site end with a purchase, while others do not.\n",
    "The dataset contains 18 features, where 10 are numerical and 8 are categorical attributes, there is also the 'Revenue' attribute, that indicates whether the session ends with shopping or not and this could be used as the class label.\n",
    "\n",
    "The columns containing users’s attributes are described in the following:\n",
    "\n",
    "\n",
    "*  \"Administrative\", \"Administrative_Duration\", \"Informational\", \"Informational _Duration\", \"ProductRelated\" and \"ProductRelated_Duration\" represent the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real time when a user takes an action, e.g. moving from one page to another. \n",
    "*  \"BounceRates\" for a web page refers to the percentage of visitors who enter the site from that page and then leave (\"bounce\") without triggering any other requests to the analytics server during that session.\n",
    "*  \"ExitRates\" for a specific web page is calculated as for all pageviews to the page, the percentage that were the last in the session.\n",
    "*  \"PageValues\" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. \n",
    "\n",
    "The previous three features represent the metrics measured by *Google Analytics* for each page in the e-commerce site. \n",
    "*   \"SpecialDay\" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentine’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8.\n",
    "*   \"Month\" indicates the month in which the session took place.\n",
    "*   \"OperatingSystems\" and \"Browser\" are features indicating which operating system and browser are used by categorical values.\n",
    "*   \"Region\" indicates the geographic region of the user. \n",
    "*   \"TrafficType\" is a particular identifier type for any hierarchy of customer base: *user* or *account* are two of the most commonly defined traffic types. (In the dataset this is a categorical feature).\n",
    "*   \"VisitorType\" states the nature of the user as returning or new visitor.\n",
    "*   \"Weekend\" features indicates whether the session's date is weekend or not.\n",
    "\n",
    "As mentioned earlier, the boolean attribute \"Revenue\" is used to understand if the session in question ends with a purchase (and was therefore successful) or if instead it was limited to a search or a quick look at the products. The analysis of the users' behaviour could be very useful to predict it and to provide a better organisation of the site (e.g. a restyiling with more pop-ups that encourage purchase by users or more offers) in order to avoid leaving the site without shopping.\n",
    "\n",
    "The analysis carried out consists of a supervised classification problem, using the Python language and Jupyter Notebook. \n",
    "\n",
    "Among the main packages imported for the purposes of our analysis are to be reported: \n",
    "-  *pandas*: an open-source library prviding high-performance data structures and data analysis tools for manipulating numerical tables and time series.\n",
    "-  *numpy*: the fundamental package for scientific computing with Python.\n",
    "NumPy provides among other things: a powerful N-dimensional array object, sophisticated (broadcasting) functions, useful linear algebra and random number capabilities.\n",
    "-  *sklearn*: a free machine learning library providing tools for data mining.It features various algorithms like support vector machine, random forests, and k-neighbours.\n",
    "-  *matplotlib*: a comprehensive library for creating static, animated, and interactive visualizations.\n",
    "-  *seaborn*: a Python data visualization library based on *matplotlib*, providing an interface for statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "F2L86zXILz6R",
    "outputId": "e83ab22f-d6ab-4b6f-8993-e6aabe6cb7ca"
   },
   "outputs": [],
   "source": [
    "# TESINA DATA SPACES\n",
    "\n",
    "# Import libraries\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import sklearn\n",
    "from sklearn import decomposition, preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO \n",
    "\n",
    "from IPython.display import Image  \n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True) # to show plots in notebook\n",
    "\n",
    "\n",
    "# online plotly\n",
    "import chart_studio\n",
    "from chart_studio import plotly\n",
    "import plotly.graph_objects as go\n",
    "from chart_studio.plotly import plot, iplot\n",
    "chart_studio.tools.set_credentials_file(username='martinaalutto', api_key='E99v9jtQ5PAAKwnMXzzf')\n",
    "\n",
    "# offline plotly\n",
    "#from plotly.offline import plot, iplot\n",
    "\n",
    "# do not show any warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set the seed for the analysis\n",
    "SEED = 40\n",
    "\n",
    "# pandas option for the output style \n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8nBuRN8MDWz8"
   },
   "source": [
    "**Exploration and Preprocessing**\n",
    "\n",
    "After importing the dataset, we can move on to exploring it, we can take a look of teh structure of our data, checking whether missing values are present in the dataset or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "1ruGo7rso91D",
    "outputId": "d193312c-d5d3-4ed5-9be0-e783e1f12901"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('/Users/Martina/Google Drive/online_shoppers_intention.csv') \n",
    "\n",
    "print('\\nDataset dimensions : ', data.shape)\n",
    "\n",
    "# Data analysis\n",
    "# Preview the first 5 lines of the loaded data \n",
    "print('This is a preview of the first 5 lines of the loaded dataset.\\n')\n",
    "print(data.head(5))\n",
    "\n",
    "#with open('/content/drive/My Drive/Colab Notebooks/Data Spaces/online_shoppers_intention.csv', newline='') as csvfile:\n",
    "#   lettore = csv.reader(csvfile, delimiter=\",\")\n",
    "#   header = next(lettore)\n",
    "#   print('\\nFeatures : ', header)\n",
    "\n",
    "# check for null values in the dataset\n",
    "print(\"\\nThere are \" + (\"some\" if data.isnull().values.any() else \"no\")  + \" null/missing values in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IwfA-8SmRYg0"
   },
   "source": [
    "In order to investigate the pair-wise correlations between two variables X and Y, we use the Pearson correlation. \n",
    "Let σ(X), σ(Y) be the standard deviation of X,Y and the covariance cov(X,Y) = E[(X−E[X])(Y−E[Y])]. \n",
    "Then we can define the Pearson correlation as ρ_{X,Y}=cov(X,Y)σ(X)σ(Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "a7EFdOkFRVUI",
    "outputId": "7806b343-1185-409b-a165-1f3d520554e1"
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "print('Correlation matrix between the dataset features.')\n",
    "corr = data.corr()\n",
    "ax = sns.heatmap(corr, vmin=-1, vmax=1, center=0, cmap=sns.diverging_palette(20, 220, n=200), square=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHEienLdGXMA"
   },
   "source": [
    "The correlation matrix graph shows a strong positive correlation between the attributes \"BounceRates\" and \"ExitRates\". Let us remember that Bounce rate is the percentage of people who landed on a page and immediately left, so they are always one-page sessions. A high Bounce rate on a home page is usually a sign that something is wrong, but it’s really a matter of context. Instead Exit rate is the percentage of people who left your site from that page and exits may have viewed more than one page in a session. That means they may not have landed on that page, but simply found their way to it through site navigation. What is important is that like Bounce rates, high Exit rates can often reveal problem areas on your site and that's why we have a strong correlation between them. \n",
    "\n",
    "It's also clear that the attributes \"Administrative\", \"Informational\" and \"ProductRelated\" are quite positively correlated with their duration attributes, because the number of pages of a certain type visited during the session is related to the time spent on that type of pages. You can see a greater correlation in the case of product pages because, with the same number of pages of the 3 types, more time is spent on a product page where all its specifications and characteristics are looked at.\n",
    "\n",
    "We can also observe a quite high positive correlation between the \"PageValues\" feature and the label \"Revenue\", and this is what we expected because the objective of the first value is to give an idea of the page that has contributed most to the site's revenue. If a page has not been involved in any way in the e-commerce transaction, its Page Value is € 0, since the page has never been visited in a session where a transaction was made.\n",
    "-- **vedere se aggiungere altro** -- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CP3QTEtmmZkB"
   },
   "source": [
    "We can observe how the categorical attributes \"Month\", \"VisitorType\" and \"Weekend\" have string values but they can be easily transformed into numbers.\n",
    "\"Weekend\" is transformed by placing *0* if it was *False* and *1* if it was *True*; similarly \"VisitorType\" is set to *1* if it was a returning visitor and *0* if it is a new one. \n",
    "Instead \"Month\" could be transformed using LabelEncoder which automatically converts each distinct label into an unique integer. However, we must note that there are no user sessions in January and April, so we transform this attribute by hand in order to avoid future problems of decoding and understanding our results. That way we'll have an encoding of months one through 12. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "SA7cbs1Q-5m5",
    "outputId": "d1df639a-907d-462a-83af-72e7323209ca"
   },
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "data['Weekend'] = data['Weekend'].apply(lambda x: 0 if x=='False' else 1)\n",
    "data['VisitorType'] = data['VisitorType'].apply(lambda x: 0 if x=='New_Visitor' else 1)\n",
    "\n",
    "for i in range(len(data)):\n",
    "        if data['Month'][i] == 'Jan':\n",
    "            data.at[i, 'Month'] = 1\n",
    "        if data['Month'][i] == 'Feb':\n",
    "            data.at[i, 'Month'] = 2\n",
    "        if data['Month'][i] == 'Mar':\n",
    "            data.at[i, 'Month'] = 3\n",
    "        if data['Month'][i] == 'Apr':\n",
    "            data.at[i, 'Month'] = 4\n",
    "        if data['Month'][i] == 'May':\n",
    "            data.at[i, 'Month'] = 5\n",
    "        if data['Month'][i] == 'June':\n",
    "            data.at[i, 'Month'] = 6\n",
    "        if data['Month'][i] == 'Jul':\n",
    "            data.at[i, 'Month'] = 7\n",
    "        if data['Month'][i] == 'Aug':\n",
    "            data.at[i, 'Month'] = 8\n",
    "        if data['Month'][i] == 'Sep':\n",
    "            data.at[i, 'Month'] = 9\n",
    "        if data['Month'][i] == 'Oct':\n",
    "            data.at[i, 'Month'] = 10\n",
    "        if data['Month'][i] == 'Nov':\n",
    "            data.at[i, 'Month'] = 11\n",
    "        if data['Month'][i] == 'Dec':\n",
    "            data.at[i, 'Month'] = 12\n",
    "\n",
    "print('This is a preview of the first 5 lines of the loaded dataset with the transformed attributes.\\n')\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "3qLmnL9POGGU",
    "outputId": "7b590276-18d8-4754-b762-bc5964ce280c"
   },
   "outputs": [],
   "source": [
    "print('The following statistical measures can give us a statistical overview of the data.\\n')\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_k9ifuFq06F"
   },
   "source": [
    "In order to better understand this data, let us move on to a more in-depth exploration, but we can already see that the all users are 'Returning' and the dataset includes only weekend sessions; these could be limitations in terms of a deeper analysis of this kind of data but we will make a general predictive analysis, a supervised classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "colab_type": "code",
    "id": "7VntY184rmuW",
    "outputId": "9c1b9b44-b5cb-48ab-ccb1-3a1283daf040"
   },
   "outputs": [],
   "source": [
    "revenue_dict = {False: \"No Revenue\", True: \"Revenue\"}\n",
    "y = data[\"Revenue\"].value_counts()\n",
    "\n",
    "d = [go.Bar(x=[revenue_dict[x] for x in y.index], y=y.values, marker=dict(color=['#FF3333','#3399FF']))]\n",
    "layout = go.Layout(\n",
    "    title='Revenue distribution over the total',\n",
    "    autosize=False,\n",
    "    width=400,\n",
    "    height=400,\n",
    "    yaxis=dict(\n",
    "        title='Number of samples',\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=d, layout=layout)\n",
    "fig.show()\n",
    "#iplot(fig, filename='basic-bar3')\n",
    "\n",
    "revenue_percentage = data[\"Revenue\"].sum() * 100 / data[\"Revenue\"].shape[0]\n",
    "print(\"The Revenue percentage is %.3f%%.\" % revenue_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "uEZe4Mc0oVir",
    "outputId": "87e1f01b-581b-4ba2-dff9-a20631a7b615"
   },
   "outputs": [],
   "source": [
    "month_revenue = data.groupby([\"Month\", \"Revenue\"]).size().unstack()\n",
    "trace1 = go.Bar(\n",
    "    x=month_revenue.index,\n",
    "    y=month_revenue[0],\n",
    "    name='No Revenue',\n",
    "    marker = dict(color = '#FF3333')\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    x=month_revenue.index,\n",
    "    y=month_revenue[1],\n",
    "    name='Revenue',\n",
    "    marker = dict(color = '#3399FF')\n",
    ")\n",
    "d2 = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Revenue Distribution for Month',\n",
    "    autosize=True,\n",
    "    barmode='stack',\n",
    "    margin=go.layout.Margin(l=50, r=50),\n",
    "    xaxis=dict(title='Month',),\n",
    "    yaxis=dict(title='Number of samples',automargin=True,),\n",
    "    legend=dict(x=0, y=1, ),\n",
    ")\n",
    "fig = go.Figure(data=d2, layout=layout)\n",
    "fig.show()\n",
    "\n",
    "#revenue_percentage = data[\"Revenue\"].sum() * 100 / data[\"Revenue\"].shape[0]\n",
    "#print(\"The Revenue percentage is %.3f%%.\" % revenue_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, there are no sessions during the months of January and April, while we can see strong traffic during the months of May (a total of 3364 sessions, with about 10% of purchases) and November (a total of 2998 sessions and about 25% of purchases).\n",
    "These data can reflect a higher online traffic due to the arrival of summer (in the case of May) and especially in view of Christmas (as far as November is concerned) where we can also notice a higher percentage of purchases, accompanied by a fairly high figure also in December. We have always to take into account the shipping time required for each online order, so the peak of purchases will obviously not be on the 24th or the 25th of December but between mid-November and early December. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "colab_type": "code",
    "id": "FdCQyXw7ow2-",
    "outputId": "e9f95b55-5fb8-4d8f-adc4-c7420d9748fc"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "revenue = data[data['Revenue'] == 1]\n",
    "no_revenue =  data[data['Revenue'] == 0]\n",
    "\n",
    "def create_revenue_trace(col, visible=False):\n",
    "    return go.Histogram(\n",
    "        x=revenue[col],\n",
    "        name='revenue',\n",
    "        marker = dict(color = '#3399FF'),\n",
    "        visible=visible,\n",
    "    )\n",
    "\n",
    "def create_no_revenue_trace(col, visible=False):\n",
    "    return go.Histogram(\n",
    "        x=no_revenue[col],\n",
    "        name='no revenue',\n",
    "        marker = dict(color = '#FF3333'),\n",
    "        visible = visible,\n",
    "    )\n",
    "\n",
    "features_for_hist = ['BounceRates', 'ExitRates', 'PageValues', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration']\n",
    "active_idx = 0\n",
    "traces_revenue = [(create_revenue_trace(col) if i != active_idx else create_revenue_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n",
    "traces_no_revenue = [(create_no_revenue_trace(col) if i != active_idx else create_no_revenue_trace(col, visible=True)) for i, col in enumerate(features_for_hist)]\n",
    "d = traces_revenue + traces_no_revenue\n",
    "\n",
    "n_features = len(features_for_hist)\n",
    "steps = []\n",
    "for i in range(n_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * len(d)],\n",
    "        label = features_for_hist[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active = active_idx,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    yaxis=dict(\n",
    "        title='#samples',\n",
    "        automargin=True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig =  go.Figure(data=d, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_box_revenue_trace(col, visible=False):\n",
    "    return go.Box(\n",
    "        y=revenue[col],\n",
    "        name='Revenue',\n",
    "        marker = dict(color = '#3399FF'),\n",
    "        visible=visible,\n",
    "    )\n",
    "\n",
    "def create_box_no_revenue_trace(col, visible=False):\n",
    "    return go.Box(\n",
    "        y=no_revenue[col],\n",
    "        name='No Revenue',\n",
    "        marker = dict(color = '#FF3333'),\n",
    "        visible = visible,\n",
    "    )\n",
    "\n",
    "# remove features with too less distinct values (e.g. binary features), because boxplot does not make any sense for them\n",
    "features_for_box = [col for col in features_for_hist if len(revenue[col].unique())>5]\n",
    "\n",
    "active_idx = 0\n",
    "box_traces_revenue = [(create_box_revenue_trace(col) if i != active_idx else create_box_revenue_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\n",
    "box_traces_no_revenue = [(create_box_no_revenue_trace(col) if i != active_idx else create_box_no_revenue_trace(col, visible=True)) for i, col in enumerate(features_for_box)]\n",
    "d = box_traces_revenue + box_traces_no_revenue\n",
    "\n",
    "n_features = len(features_for_box)\n",
    "steps = []\n",
    "for i in range(n_features):\n",
    "    step = dict(\n",
    "        method = 'restyle',  \n",
    "        args = ['visible', [False] * len(d)],\n",
    "        label = features_for_box[i],\n",
    "    )\n",
    "    step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
    "    step['args'][1][i + n_features] = True # Toggle i'th trace to \"visible\"\n",
    "    steps.append(step)\n",
    "\n",
    "sliders = [dict(\n",
    "    active = active_idx,\n",
    "    currentvalue = dict(\n",
    "        prefix = \"Feature: \", \n",
    "        xanchor= 'center',\n",
    "    ),\n",
    "    pad = {\"t\": 50},\n",
    "    steps = steps,\n",
    "    len=1,\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    sliders=sliders,\n",
    "    yaxis=dict(\n",
    "        title='value',\n",
    "        automargin=True,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=d, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "colab_type": "code",
    "id": "tqDh0NhPwd2q",
    "outputId": "74806a05-377d-47ba-b73b-04e26d417ae5"
   },
   "outputs": [],
   "source": [
    "data[['PageValues', 'Revenue']].groupby(['Revenue'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNGqyObSxEMC"
   },
   "source": [
    "Calculating the average of the attribute \"PagesValues\" for both sessions ended with purchase and sessions ended without purchase, we can see that the former is much higher than the latter. This confirms the quite good correlation that we noticed previously between the attributes \"PagesValues\" and \"Revenue\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uED9LZkykJp"
   },
   "source": [
    "Now we can save the attribute \"Revenue\" as label and exlude it from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LRlR8BRItoSB"
   },
   "outputs": [],
   "source": [
    "# Save attribute label and delete it from the dataset\n",
    "label = data['Revenue'][:]\n",
    "X = np.array(data.drop(['Revenue'], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "54swQrUAzEcy"
   },
   "source": [
    "In order to reduce the dimensionality of our dataset, we could apply the PCA (Principal Component Analysis), i.e. a technique for deriving a low-dimensional set of features from a large set of variables. Given data points in a large space, it projects them into a lower dimensional space while preserving as much information as possible and maximizing the variance of the projected data. The chosen projection is the one that minimizes mean squared distance between data point and projections. The new space's principal component corrensponds to the points in the direction of the largest variance; each sbusequent principal component is orthogonal to the previous ones and it corresponds to points in the directions of the largest variance of the residual subspace. \n",
    "\n",
    "Given a set of data points, if we compute the covariance matrix E, the PCA basis vectors are the eigenvectors of E, therefore the larger eigenvalue corresponds to the more important eigenvectors. \n",
    "\n",
    "As it is a required condition for PCA analysis, we firstly have to standardize data, by subtracting the mean and scaling to unit variance with the *StandardScaler* of the sklearn library.\n",
    "\n",
    "Once have scaled data, we can perform the PCA and the amount of explained variance by each component and the cumulative explained variance in order to can choose in order to choose the most appropriate number of components for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZonnivGf6eFe"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(random_state=SEED)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "tot = sum(pca.explained_variance_) # total explained variance of all principal components\n",
    "var_exp = [(i / tot) * 100 for i in sorted(pca.explained_variance_, reverse=True)] # individual explained variance\n",
    "cum_var_exp = np.cumsum(var_exp) # cumulative explained variance\n",
    "\n",
    "trace_cum_var_exp = go.Bar(\n",
    "    x=list(range(1, len(cum_var_exp) + 1)), \n",
    "    y=var_exp,\n",
    "    marker = dict(color = '#00CCCC'),\n",
    "    name=\"individual explained variance\",\n",
    ")\n",
    "trace_ind_var_exp = go.Bar(\n",
    "    x=list(range(1, len(cum_var_exp) + 1)),\n",
    "    y=cum_var_exp,\n",
    "    marker = dict(color = '#FF9999'),\n",
    "    name=\"cumulative explained variance\",\n",
    ")\n",
    "d = [trace_cum_var_exp, trace_ind_var_exp]\n",
    "layout = go.Layout(\n",
    "    title='Individual and Cumulative Explained Variance',\n",
    "    autosize=True,\n",
    "    yaxis=dict(\n",
    "        title='percentage of explained variance',\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title=\"principal components\",\n",
    "        dtick=1,\n",
    "    ),\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "    ),\n",
    ")\n",
    "fig = go.Figure(data=d, layout=layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQbLHeWAzi1e"
   },
   "source": [
    "We can see from the graph that the first four components explain about half of the total variance, instead the last three components explain near no variance. \n",
    "Covering more than 84% of the total variance explained, we can choose a PCA with 10 components. \n",
    "\n",
    "From now on, we will carry out two types of analysis, the first taking into account the dimensional reduction implemented by the PCA and the second where the classifiers will be used directly on the original data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "pca = PCA(n_components, random_state=SEED)\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "principal_X = pd.DataFrame(data = data_pca, columns = [\"Principal component%d\" % (x + 1) for x in range(n_components)])\n",
    "final_X = pd.concat([principal_X, label], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can move on the random split of data into train and test sets in proportion 7:3. To do this, we can use the function train_test_split of the library scikit-learn, by setting\n",
    "the parameter random_state in order to fix the seed of the random number generator.\n",
    "This function splits the data into two sets by a given proportion that will be used for our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, target_train, target_test = train_test_split(X, label, test_size=0.3, train_size=0.7, random_state = SEED)\n",
    "X_train_pca, X_test_pca, target_train_pca, target_test_pca = train_test_split(principal_X, label, test_size=0.3, train_size=0.7, random_state = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification**\n",
    "\n",
    "**K Nearest Neighbours**\n",
    "\n",
    "Having divided data into three random subsets, we’ll proceed with the K-Nearest Neighbors classification by using the sklearn library’s KNeighborsClassifier.\n",
    "\n",
    "The K-Nearest Neighbors classifier computes the distance of each training record to others, it identifies k nearest neighbors and it uses class labels of nearest neighbors to determine the class label of the unknown record.\n",
    "The parameter *n_neighbors* of the KNeighborsClassifier corresponds to K, the number of nearest neighbors considered for the prediction; instead the parameter *weights* can be *uniform*, then all neighbors have the same weight for the voting or *distance*, then the votes of the neighbors are weighted by the inverse of the distance for the voting.\n",
    "\n",
    "Let us perform this classifier on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pNfHxxhok1XN",
    "outputId": "bccce793-e941-44eb-9ef9-1efd28d8417e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#KNN\n",
    "max_accuracy = 0\n",
    "max_neigh = 0\n",
    "max_weight = ''\n",
    "#Create KNN Classifier\n",
    "for k in [1, 5, 10, 15, 20, 30, 40, 50]:\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        knn.fit(X_train, target_train)\n",
    "\n",
    "        #Predict the response for test dataset\n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        if (max_accuracy <= metrics.accuracy_score(target_test_pca, y_pred)):\n",
    "            max_accuracy = metrics.accuracy_score(target_test_pca, y_pred)\n",
    "            max_neigh = k\n",
    "            max_weight = weights\n",
    "        \n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Best Accuracy for KNN on the full dataset with k =\",max_neigh,\"and\",max_weight,\"weights :\",max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we perform the KNeighborsClassifier on the dataset reduced by considering the first ten principal components of the PCA, this will be the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "max_neigh = 0\n",
    "max_weight = ''\n",
    "for k in [1, 5, 10, 15, 20, 30, 40, 50]:\n",
    "    for weights in ['uniform', 'distance']:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=weights)\n",
    "\n",
    "        #Train the model using the training sets\n",
    "        knn.fit(X_train_pca, target_train_pca)\n",
    "        #Predict the response for test dataset\n",
    "        y_pred_pca = knn.predict(X_test_pca)\n",
    "        \n",
    "        if (max_accuracy <= metrics.accuracy_score(target_test_pca, y_pred_pca)):\n",
    "            max_accuracy = metrics.accuracy_score(target_test_pca, y_pred_pca)\n",
    "            max_neigh = k\n",
    "            max_weight = weights\n",
    "        \n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Best Accuracy for KNN on the reduced dataset with k =\",max_neigh,\"and\",max_weight,\"weights :\",max_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the best value for the accuracy is k = 15 with uniform weights and the accuracy value of 87.4% is greater than the one obtained performing the classifier on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**\n",
    "\n",
    "With the classification tree we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs. In interpreting the results of a classification tree, we are often interested not only in the class prediction corresponding to a particular terminal node region, but also in the class proportions among the training observations that fall into that region.\n",
    "\n",
    "The classification error rate is simply the fraction of the training observations in that region that do not belong to the most common class. \n",
    "However can define the *Gini* index by $$G = \\sum_{k=1}^{2} p_{mk}(1-p_{mk})$$ a measure of total variance across the two classes.\n",
    "The $p_{mk}$ represents the proportion of training observations in the m-th region that are from the k-th class. The Gini index is small if all of the $p_{mk}$'s are close to 0 or 1, namely that a node contains predominantly observations from a single class.\n",
    "An alternative to the *Gini* index is *cross-entropy*, given by\n",
    "$$D = -\\sum_{k=1}^{2} p_{mk}log(p_{mk})$$. This index will take on a value near zero if the $p_{mk}$ are all near 0 or near 1. When building a classification tree, either the *Gini* index or the *cross-entropy* are typically used to evaluate the quality of a particular split, since these two approaches are more sensitive to node purity than is the classification error rate.\n",
    "\n",
    "We firstly perform the classifier on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HYuqhgKysFVW",
    "outputId": "d0fcb43e-4faf-44db-8b1b-b69f30eb039c"
   },
   "outputs": [],
   "source": [
    "# DECISION TREE\n",
    "max_accuracy = 0\n",
    "max_dep = 0\n",
    "max_crit = ''\n",
    "max_samp = 0\n",
    "for depth in [3, 10, 25, 40, 65]:\n",
    "    for crit in ['gini', 'entropy']:\n",
    "        for samples in [2, 5, 10, 15]:\n",
    "            \n",
    "            clf = tree.DecisionTreeClassifier(criterion=crit, max_depth=depth, min_samples_split=samples)\n",
    "            clf = clf.fit(X_train, target_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            if (max_accuracy<= metrics.accuracy_score(target_test, y_pred)):\n",
    "                max_accuracy = metrics.accuracy_score(target_test, y_pred)\n",
    "                max_dep = depth\n",
    "                max_crit = crit\n",
    "                max_samp = samples\n",
    "\n",
    "print(\"Best Accuracy for DecisionTree on the full dataset\\nwith criterion of\",max_crit,\",min_samples_split =\",max_samp,\",max_depth =\",max_dep,\":\",max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "colab_type": "code",
    "id": "foQa4j-9w1DH",
    "outputId": "94524ec2-d131-4940-c9fe-7916ab0a26a1"
   },
   "outputs": [],
   "source": [
    "feature_cols = ['Administrative', 'Administrative_Duration', 'Informational', 'Informational_Duration', 'ProductRelated', 'ProductRelated_Duration', 'BounceRates', 'ExitRates', 'PageValues', 'SpecialDay', 'Month','OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(tree.DecisionTreeClassifier(criterion=max_crit, max_depth=max_dep, min_samples_split=max_samp).fit(X_train, target_train), out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['True','False'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('revenues_depth3.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Kvc_8NOBw9HH",
    "outputId": "6497e2d9-aead-43f6-dff9-4ce67f4747f3"
   },
   "outputs": [],
   "source": [
    "max_accuracy = 0\n",
    "max_dep = 0\n",
    "max_crit = ''\n",
    "max_samp = 0\n",
    "for depth in [3, 10, 25, 40, 65]:\n",
    "    for crit in ['gini', 'entropy']:\n",
    "        for samples in [2, 5, 10, 15]:\n",
    "            \n",
    "            clf = tree.DecisionTreeClassifier(criterion=crit, max_depth=depth, min_samples_split=samples)\n",
    "            clf = clf.fit(X_train_pca, target_train_pca)\n",
    "\n",
    "            y_pred_pca = clf.predict(X_test_pca)\n",
    "            if (max_accuracy<= metrics.accuracy_score(target_test_pca, y_pred_pca)):\n",
    "                max_accuracy = metrics.accuracy_score(target_test_pca, y_pred_pca)\n",
    "                max_dep = depth\n",
    "                max_crit = crit\n",
    "                max_samp = samples\n",
    "\n",
    "print(\"Best Accuracy for DecisionTree on the reduced dataset\\nwith criterion of\",max_crit,\",min_samples_split =\",max_samp,\",max_depth =\",max_dep,\":\",max_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 932
    },
    "colab_type": "code",
    "id": "XM8oYyzXyDA0",
    "outputId": "b4256e0f-fd44-4fbb-c089-1908db5c0cec"
   },
   "outputs": [],
   "source": [
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = feature_cols,class_names=['True','False'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.write_png('revenues_depth5.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "soejUwkWy7oC",
    "outputId": "a010e3de-c180-4ce7-baed-694487ef5ef5"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, target_train, target_test = train_test_split(X, label, test_size=0.5, train_size=0.5, random_state = 0)\n",
    "\n",
    "clf_svm = svm.SVC(kernel = 'linear', gamma='auto')\n",
    "clf_svm.fit(X_train, target_train)\n",
    "\n",
    "y_pred = clf_svm.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "O82YuH30osWf",
    "outputId": "a2faf997-e8ab-43e7-d075-563610d2becf"
   },
   "outputs": [],
   "source": [
    "accuracy = np.zeros((7,2))\n",
    "i = 0\n",
    "for c in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
    "  j = 0\n",
    "  for ker in ['linear', 'rbf']:\n",
    "    # Create an instance of SVC Classifier with linear kernel and fit the data.\n",
    "    clf_svm = svm.SVC(C = c, kernel = ker, gamma='auto')\n",
    "    clf_svm.fit(X_train, target_train)\n",
    "\n",
    "    y_pred = clf_svm.predict(X_test)\n",
    "    accuracy[i,j] = metrics.accuracy_score(target_test, y_pred)\n",
    "    j += 1\n",
    "  i += 1\n",
    "  \n",
    "print(accuracy)\n",
    "print(accuracy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "DxHn1mLA5rxA",
    "outputId": "6627beaf-23b5-4320-ac06-4b98062c9950"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logreg.predict(X_test)\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBc9pwaO7aI-"
   },
   "outputs": [],
   "source": [
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Tesina_Data_Spaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
